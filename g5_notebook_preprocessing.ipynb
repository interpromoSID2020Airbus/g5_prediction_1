{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Groupe 5 - Prediction\n",
    "> Preprocessing Notebook\n",
    "\n",
    "<span class=\"tocSkip\"></span>\n",
    "> *Authors : All*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Environment\" data-toc-modified-id=\"Environment-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Environment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Data-loading\" data-toc-modified-id=\"Data-loading-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Data loading</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Functions</a></span></li></ul></li><li><span><a href=\"#Textual-analysis-approach\" data-toc-modified-id=\"Textual-analysis-approach-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Textual analysis approach</a></span><ul class=\"toc-item\"><li><span><a href=\"#V0\" data-toc-modified-id=\"V0-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>V0</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1-:-ALL_DATA-Preprocessing\" data-toc-modified-id=\"Step-1-:-ALL_DATA-Preprocessing-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Step 1 : ALL_DATA Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading\" data-toc-modified-id=\"Loading-3.1.1.1\"><span class=\"toc-item-num\">3.1.1.1&nbsp;&nbsp;</span>Loading</a></span></li><li><span><a href=\"#Dropping\" data-toc-modified-id=\"Dropping-3.1.1.2\"><span class=\"toc-item-num\">3.1.1.2&nbsp;&nbsp;</span>Dropping</a></span></li><li><span><a href=\"#Cleaning\" data-toc-modified-id=\"Cleaning-3.1.1.3\"><span class=\"toc-item-num\">3.1.1.3&nbsp;&nbsp;</span>Cleaning</a></span></li><li><span><a href=\"#Exporting\" data-toc-modified-id=\"Exporting-3.1.1.4\"><span class=\"toc-item-num\">3.1.1.4&nbsp;&nbsp;</span>Exporting</a></span></li></ul></li><li><span><a href=\"#Step-2-:--TF-IDF-preprocessing\" data-toc-modified-id=\"Step-2-:--TF-IDF-preprocessing-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Step 2 :  TF-IDF preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading\" data-toc-modified-id=\"Loading-3.1.2.1\"><span class=\"toc-item-num\">3.1.2.1&nbsp;&nbsp;</span>Loading</a></span></li><li><span><a href=\"#Dropping\" data-toc-modified-id=\"Dropping-3.1.2.2\"><span class=\"toc-item-num\">3.1.2.2&nbsp;&nbsp;</span>Dropping</a></span></li><li><span><a href=\"#Computing\" data-toc-modified-id=\"Computing-3.1.2.3\"><span class=\"toc-item-num\">3.1.2.3&nbsp;&nbsp;</span>Computing</a></span></li><li><span><a href=\"#Exporting\" data-toc-modified-id=\"Exporting-3.1.2.4\"><span class=\"toc-item-num\">3.1.2.4&nbsp;&nbsp;</span>Exporting</a></span></li></ul></li></ul></li><li><span><a href=\"#V1\" data-toc-modified-id=\"V1-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>V1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading\" data-toc-modified-id=\"Loading-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Loading</a></span></li><li><span><a href=\"#Cleaning\" data-toc-modified-id=\"Cleaning-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Cleaning</a></span></li><li><span><a href=\"#Features-engineering\" data-toc-modified-id=\"Features-engineering-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Features engineering</a></span></li><li><span><a href=\"#Model-pre-processing\" data-toc-modified-id=\"Model-pre-processing-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Model pre processing</a></span></li><li><span><a href=\"#Exporting\" data-toc-modified-id=\"Exporting-3.2.5\"><span class=\"toc-item-num\">3.2.5&nbsp;&nbsp;</span>Exporting</a></span></li></ul></li></ul></li><li><span><a href=\"#Metadata-analysis-approach\" data-toc-modified-id=\"Metadata-analysis-approach-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Metadata analysis approach</a></span><ul class=\"toc-item\"><li><span><a href=\"#V0\" data-toc-modified-id=\"V0-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>V0</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning\" data-toc-modified-id=\"Cleaning-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Cleaning</a></span></li><li><span><a href=\"#Filling-the-DataFrame\" data-toc-modified-id=\"Filling-the-DataFrame-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Filling the DataFrame</a></span></li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Feature engineering</a></span></li><li><span><a href=\"#Exporting\" data-toc-modified-id=\"Exporting-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Exporting</a></span></li></ul></li><li><span><a href=\"#V1.1\" data-toc-modified-id=\"V1.1-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>V1.1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning\" data-toc-modified-id=\"Cleaning-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Cleaning</a></span></li><li><span><a href=\"#Filling-DataFrame\" data-toc-modified-id=\"Filling-DataFrame-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Filling DataFrame</a></span></li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Feature engineering</a></span></li><li><span><a href=\"#Exporting\" data-toc-modified-id=\"Exporting-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Exporting</a></span></li></ul></li><li><span><a href=\"#V1.2\" data-toc-modified-id=\"V1.2-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>V1.2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning\" data-toc-modified-id=\"Cleaning-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Cleaning</a></span></li><li><span><a href=\"#Filling-the-DataFrame\" data-toc-modified-id=\"Filling-the-DataFrame-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Filling the DataFrame</a></span></li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Feature engineering</a></span></li><li><span><a href=\"#Exporting\" data-toc-modified-id=\"Exporting-4.3.4\"><span class=\"toc-item-num\">4.3.4&nbsp;&nbsp;</span>Exporting</a></span></li></ul></li><li><span><a href=\"#V2\" data-toc-modified-id=\"V2-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>V2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning\" data-toc-modified-id=\"Cleaning-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Cleaning</a></span></li><li><span><a href=\"#Filling-DataFrame\" data-toc-modified-id=\"Filling-DataFrame-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Filling DataFrame</a></span></li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-4.4.3\"><span class=\"toc-item-num\">4.4.3&nbsp;&nbsp;</span>Feature engineering</a></span></li><li><span><a href=\"#Exporting\" data-toc-modified-id=\"Exporting-4.4.4\"><span class=\"toc-item-num\">4.4.4&nbsp;&nbsp;</span>Exporting</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook's aim is to prepare every DataFrames that will be needed for model building.\n",
    "\n",
    "It is separeted in 3 main parts : \n",
    "- The first one is the environnement setting, with imports of libraries and functions, and data loading.\n",
    "- The second one gathers preprocess for the textual analysis approach. It means that the output DataFrames will be needed for each version of models for this approach.\n",
    "- The last one is the equivalent but for the metadata analysis approach.\n",
    "\n",
    "In every cell comments can be found about what is done into it. \n",
    "\n",
    "Some explaining cells will also be used sometimes in order to interprate, or conclude about the previous code cell.\n",
    "\n",
    "Please, <span style= 'background:red'> don't run this Notebook </span>, it's execution is really long (few hours). Files that are outputs of this notebook are included in repository, use them to run the result Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T12:29:28.185552Z",
     "start_time": "2020-01-17T12:29:26.242702Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm_notebook\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from warnings import filterwarnings\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T12:29:28.239618Z",
     "start_time": "2020-01-17T12:29:28.231600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T12:29:30.485100Z",
     "start_time": "2020-01-17T12:29:28.273710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\laura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.downloader.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T12:29:50.575427Z",
     "start_time": "2020-01-17T12:29:30.514177Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"Files_to_load/\"\n",
    "\n",
    "all_data = pd.read_excel(path + \"ALL_DATA.xlsx\")\n",
    "aircraft = pd.read_csv(path + \"SEATGURU_INFO_AIRCRAFT.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T12:29:54.239093Z",
     "start_time": "2020-01-17T12:29:50.941402Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.full.g5_notebook_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Textual analysis approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, preprocessing consists in :\n",
    "- dropping empty commentaries\n",
    "- cleaning commentaries\n",
    "- computing a tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : ALL_DATA Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T13:48:06.894795Z",
     "start_time": "2020-01-14T13:47:58.499Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = all_data[all_data[\"Review\"].notnull()]\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T13:48:06.899807Z",
     "start_time": "2020-01-14T13:47:59.046Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.drop([14635, 70282, 123088, 123158, 123244], axis=0, inplace=True)\n",
    "\n",
    "i = 1\n",
    "idx_to_drop = []\n",
    "for idx, com in enumerate(train['Review'].iloc[range(139315)]):\n",
    "    if len(com.strip()) == 0:\n",
    "        idx_to_drop.append(idx)\n",
    "    elif detect(com) != 'en':\n",
    "        idx_to_drop.append(idx)\n",
    "\n",
    "train.drop(idx_to_drop, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T13:48:06.904821Z",
     "start_time": "2020-01-14T13:47:59.691Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[\"ReviewClean\"] = \"\"\n",
    "train[\"ReviewClean\"] = train[\"ReviewClean\"].apply(lambda x : preprocessing(x))\n",
    "\n",
    "data.drop(data.index[100000:136303], 0, inplace=True)\n",
    "data = TF_IDF(data, \"ReviewClean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T13:48:06.910834Z",
     "start_time": "2020-01-14T13:48:00.442Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv(path + \"ALL_DATA_PREPROCESS_V1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 :  TF-IDF preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:44:05.072644Z",
     "start_time": "2020-01-14T12:44:01.510290Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_txt_V0 = pd.read_csv(path + 'ALL_DATA_PREPROCESS_V1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T10:49:37.198879Z",
     "start_time": "2020-01-14T10:49:37.018160Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_txt_V0 = pd.DataFrame(data_txt_V0[[\"ReviewClean\",\"Seat_Comfort\",\"Food_And_Beverages\",\"Cabin_Staff_Service\"]],columns = [\"ReviewClean\",\"Seat_Comfort\",\"Food_And_Beverages\",\"Cabin_Staff_Service\"])\n",
    "\n",
    "data_txt_V0_labels = pd.DataFrame(data_txt_V0[[\"Seat_Comfort\",\"Food_And_Beverages\",\"Cabin_Staff_Service\"]],columns = [\"Seat_Comfort\",\"Food_And_Beverages\",\"Cabin_Staff_Service\"])\n",
    "\n",
    "# Dropping the half of the rows in order to run TF-IDF computing on this half\n",
    "data_txt_V0.drop(data_txt_V0.index[70000:136303],0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T10:49:39.201377Z",
     "start_time": "2020-01-14T10:49:38.355555Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying TF-IDF on pre processed data\n",
    "data_txt_V0 = TF_IDF_V0(data_txt_V0, \"ReviewClean\")\n",
    "\n",
    "data_txt_V0 = pd.concat([data_txt_V0,data_txt_V0_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T10:49:41.229518Z",
     "start_time": "2020-01-14T10:49:40.105593Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deleting all the columns but the choosen label\n",
    "\n",
    "data_seat = data_txt_V0.drop(\n",
    "    [\"Food_And_Beverages\", \"Cabin_Staff_Service\"], axis=1)\n",
    "data_food = data_txt_V0.drop([\"Seat_Comfort\", \"Cabin_Staff_Service\"], axis=1)\n",
    "data_staff = data_txt_V0.drop([\"Food_And_Beverages\", \"Seat_Comfort\"], axis=1)\n",
    "\n",
    "# Deleting missing values\n",
    "\n",
    "data_seat = data_seat[data_seat[\"Seat_Comfort\"].notnull()]\n",
    "\n",
    "data_food = data_food[data_food[\"Food_And_Beverages\"].notnull()]\n",
    "data_staff = data_staff[data_staff[\"Cabin_Staff_Service\"].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T10:50:18.315667Z",
     "start_time": "2020-01-14T10:49:44.912913Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_seat.to_csv(path + \"g5_seat_V0.csv\")\n",
    "data_food.to_csv(path + \"g5_food_V0.csv\")\n",
    "data_staff.to_csv(path + \"g5_staff_V0.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, preprocessing consists in :\n",
    "- dropping empty commentaries\n",
    "- cleaning more deeply (use of regex...)\n",
    "- computing a tfidf with only important words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:04:16.610280Z",
     "start_time": "2020-01-15T11:04:16.478209Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Charging data and converting commentaries into sentences\n",
    "\n",
    "sentences = database(all_data, \"Review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:05:26.193621Z",
     "start_time": "2020-01-15T11:04:16.843163Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:06<00:00, 17.24it/s]\n"
     ]
    }
   ],
   "source": [
    "#Charging the data\n",
    "\n",
    "sentences_clean = clean_data(sentences, \"Review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:05:30.976494Z",
     "start_time": "2020-01-15T11:05:30.842260Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice TF-IDF de ALL_DATA.xlsx enregistrÃ©e\n"
     ]
    }
   ],
   "source": [
    "# Creating and saving the tf-idf dataframe\n",
    "\n",
    "create_tfidf(sentences_clean, 'ALL_DATA.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous function creates a csv that will be processed in order to add features. We decided to proceed this way because it is a very long process to compute TF-IDF, and this way you can use directly the csv file via the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T12:30:05.331722Z",
     "start_time": "2020-01-17T12:30:04.103867Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Charging the data\n",
    "\n",
    "preprocessed_txt_V1 =  pd.read_csv(path + 'ALL_DATA_Processed_15.csv')\n",
    "all_data_txt_V1 =  all_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T12:30:06.695916Z",
     "start_time": "2020-01-17T12:30:06.451268Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping of unreadable rows and index reset\n",
    "\n",
    "preprocessed_txt_V1.drop([\"index\"], axis=1, inplace=True)\n",
    "\n",
    "all_data_txt_V1.drop([14635, 70282, 123088, 123158, 123244], axis=0, inplace=True)\n",
    "all_data_txt_V1 = all_data_txt_V1[all_data_txt_V1[\"Review\"].notnull()]\n",
    "all_data_txt_V1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T12:30:07.888492Z",
     "start_time": "2020-01-17T12:30:07.869444Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creation of the different word lists used by the functions for generating the features\n",
    "\n",
    "# Selecting the moost relevant words for the different lexical fields \n",
    "\n",
    "# Restraining the world count at a low level in order to decrease the execution time\n",
    "\n",
    "list_positive = ['amazing', 'excellent', 'great', 'satisfying', 'nice', 'pleasing',\n",
    "                 'super', 'welcome', 'useful', 'expert', 'comfortable', 'flawless',\n",
    "                 'perfect', 'awesome', 'incredible', 'wonderful', 'friendly']\n",
    "\n",
    "list_negative = ['atrocious', 'awful', 'poor', 'unacceptable', 'gross', 'bad',\n",
    "                 'deficient', 'not good', 'wrong', 'disastrous', 'painful',\n",
    "                 'terrible', 'upsetting', 'unpleasant', 'unhappy',\n",
    "                 'catastrophic', 'rude', 'impolite', 'shitty', 'dumb', 'lame']\n",
    "\n",
    "list_Staff = ['crew', 'staff', 'steward', 'security', 'stewardess']\n",
    "list_Food = ['snack', 'food', 'eat', 'drink', 'meal']\n",
    "list_Inflight = ['wifi', 'movie', 'kid', 'internet', 'screen']\n",
    "list_Seat = ['soft', 'hard', 'dirty', 'comfort', 'bed']\n",
    "\n",
    "\n",
    "punctuation_list = [',', '.', ';', '?', '!', ':']\n",
    "special_list = ['?', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T12:51:48.019588Z",
     "start_time": "2020-01-17T12:30:09.631765Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Data_Source    Airline_Name Airline_Type Region_Operation  \\\n",
      "0       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "1       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "2       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "3       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "4       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "...                         ...             ...          ...              ...   \n",
      "139310                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139311                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139312                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139313                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139314                    Kayak  zanair-limited          NaN              NaN   \n",
      "\n",
      "       Aircraft_Type    Cabin_Class Type_Of_Lounge Type_Of_Traveller  \\\n",
      "0               E120  Economy Class            NaN      Solo Leisure   \n",
      "1       Embraer E120  Economy Class            NaN      Solo Leisure   \n",
      "2                NaN  Economy Class            NaN      Solo Leisure   \n",
      "3                NaN  Economy Class            NaN    Couple Leisure   \n",
      "4             CR 900  Economy Class            NaN    Couple Leisure   \n",
      "...              ...            ...            ...               ...   \n",
      "139310           NaN            NaN            NaN               NaN   \n",
      "139311           NaN            NaN            NaN               NaN   \n",
      "139312           NaN            NaN            NaN               NaN   \n",
      "139313           NaN            NaN            NaN               NaN   \n",
      "139314           NaN            NaN            NaN               NaN   \n",
      "\n",
      "       Date_Visit      Date_Flown  ... compound nbsentence   meanword  ,   .  \\\n",
      "0             NaN       June 2019  ...  -0.7518         13  10.769231  1  12   \n",
      "1             NaN       June 2019  ...   0.7569          6  12.000000  1   5   \n",
      "2             NaN  September 2019  ...  -0.9600         10  12.600000  5   9   \n",
      "3             NaN  September 2019  ...  -0.1416          5  23.800000  6   4   \n",
      "4             NaN  September 2019  ...  -0.6106         13   9.769231  0  12   \n",
      "...           ...             ...  ...      ...        ...        ... ..  ..   \n",
      "139310        NaN             NaN  ...   0.9169          3   8.666667  0   2   \n",
      "139311        NaN             NaN  ...   0.8797          3   8.666667  0   2   \n",
      "139312        NaN             NaN  ...  -0.2732          2   7.500000  0   1   \n",
      "139313        NaN             NaN  ...  -0.1280          3  10.333333  0   2   \n",
      "139314        NaN             NaN  ...   0.0000          1   5.000000  0   0   \n",
      "\n",
      "        ;  ?  !  :  nb_ponct_normal  \n",
      "0       0  0  0  0                0  \n",
      "1       0  0  1  0                0  \n",
      "2       0  0  1  0                0  \n",
      "3       0  0  1  0                0  \n",
      "4       0  0  0  0                0  \n",
      "...    .. .. .. ..              ...  \n",
      "139310  0  0  0  1                0  \n",
      "139311  0  0  0  2                0  \n",
      "139312  0  0  0  1                0  \n",
      "139313  0  0  0  2                0  \n",
      "139314  0  0  0  1                0  \n",
      "\n",
      "[139315 rows x 58 columns]\n",
      "                    Data_Source    Airline_Name Airline_Type Region_Operation  \\\n",
      "0       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "1       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "2       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "3       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "4       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "...                         ...             ...          ...              ...   \n",
      "139310                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139311                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139312                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139313                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139314                    Kayak  zanair-limited          NaN              NaN   \n",
      "\n",
      "       Aircraft_Type    Cabin_Class Type_Of_Lounge Type_Of_Traveller  \\\n",
      "0               E120  Economy Class            NaN      Solo Leisure   \n",
      "1       Embraer E120  Economy Class            NaN      Solo Leisure   \n",
      "2                NaN  Economy Class            NaN      Solo Leisure   \n",
      "3                NaN  Economy Class            NaN    Couple Leisure   \n",
      "4             CR 900  Economy Class            NaN    Couple Leisure   \n",
      "...              ...            ...            ...               ...   \n",
      "139310           NaN            NaN            NaN               NaN   \n",
      "139311           NaN            NaN            NaN               NaN   \n",
      "139312           NaN            NaN            NaN               NaN   \n",
      "139313           NaN            NaN            NaN               NaN   \n",
      "139314           NaN            NaN            NaN               NaN   \n",
      "\n",
      "       Date_Visit      Date_Flown  ...    neg    neu    pos compound  \\\n",
      "0             NaN       June 2019  ...  0.089  0.875  0.036  -0.7518   \n",
      "1             NaN       June 2019  ...  0.030  0.853  0.118   0.7569   \n",
      "2             NaN  September 2019  ...  0.190  0.774  0.036  -0.9600   \n",
      "3             NaN  September 2019  ...  0.078  0.849  0.073  -0.1416   \n",
      "4             NaN  September 2019  ...  0.104  0.814  0.081  -0.6106   \n",
      "...           ...             ...  ...    ...    ...    ...      ...   \n",
      "139310        NaN             NaN  ...  0.000  0.583  0.417   0.9169   \n",
      "139311        NaN             NaN  ...  0.135  0.457  0.408   0.8797   \n",
      "139312        NaN             NaN  ...  0.139  0.861  0.000  -0.2732   \n",
      "139313        NaN             NaN  ...  0.053  0.947  0.000  -0.1280   \n",
      "139314        NaN             NaN  ...  0.000  1.000  0.000   0.0000   \n",
      "\n",
      "        nbsentence   meanword  nb_ponct_normal  ?  !  nb_ponct_special  \n",
      "0               13  10.769231               13  0  0                 0  \n",
      "1                6  12.000000                7  0  1                 0  \n",
      "2               10  12.600000               15  0  1                 0  \n",
      "3                5  23.800000               11  0  1                 0  \n",
      "4               13   9.769231               12  0  0                 0  \n",
      "...            ...        ...              ... .. ..               ...  \n",
      "139310           3   8.666667                3  0  0                 0  \n",
      "139311           3   8.666667                4  0  0                 0  \n",
      "139312           2   7.500000                2  0  0                 0  \n",
      "139313           3  10.333333                4  0  0                 0  \n",
      "139314           1   5.000000                1  0  0                 0  \n",
      "\n",
      "[139315 rows x 55 columns]\n",
      "                    Data_Source    Airline_Name Airline_Type Region_Operation  \\\n",
      "0       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "1       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "2       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "3       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "4       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "...                         ...             ...          ...              ...   \n",
      "139310                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139311                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139312                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139313                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139314                    Kayak  zanair-limited          NaN              NaN   \n",
      "\n",
      "       Aircraft_Type    Cabin_Class Type_Of_Lounge Type_Of_Traveller  \\\n",
      "0               E120  Economy Class            NaN      Solo Leisure   \n",
      "1       Embraer E120  Economy Class            NaN      Solo Leisure   \n",
      "2                NaN  Economy Class            NaN      Solo Leisure   \n",
      "3                NaN  Economy Class            NaN    Couple Leisure   \n",
      "4             CR 900  Economy Class            NaN    Couple Leisure   \n",
      "...              ...            ...            ...               ...   \n",
      "139310           NaN            NaN            NaN               NaN   \n",
      "139311           NaN            NaN            NaN               NaN   \n",
      "139312           NaN            NaN            NaN               NaN   \n",
      "139313           NaN            NaN            NaN               NaN   \n",
      "139314           NaN            NaN            NaN               NaN   \n",
      "\n",
      "       Date_Visit      Date_Flown  ... useful expert comfortable flawless  \\\n",
      "0             NaN       June 2019  ...      0      0           0        0   \n",
      "1             NaN       June 2019  ...      0      0           0        0   \n",
      "2             NaN  September 2019  ...      0      0           0        0   \n",
      "3             NaN  September 2019  ...      0      0           0        0   \n",
      "4             NaN  September 2019  ...      0      0           0        0   \n",
      "...           ...             ...  ...    ...    ...         ...      ...   \n",
      "139310        NaN             NaN  ...      0      0           0        0   \n",
      "139311        NaN             NaN  ...      0      0           0        0   \n",
      "139312        NaN             NaN  ...      0      0           0        0   \n",
      "139313        NaN             NaN  ...      0      0           0        0   \n",
      "139314        NaN             NaN  ...      0      0           0        0   \n",
      "\n",
      "        perfect  awesome  incredible  wonderful  friendly  nb_word_positif  \n",
      "0             0        0           0          0         0                0  \n",
      "1             0        0           0          0         0                0  \n",
      "2             0        0           0          0         0                0  \n",
      "3             0        0           0          0         0                0  \n",
      "4             0        0           0          0         0                0  \n",
      "...         ...      ...         ...        ...       ...              ...  \n",
      "139310        0        0           0          0         0                0  \n",
      "139311        0        0           0          0         1                0  \n",
      "139312        0        0           0          0         0                0  \n",
      "139313        0        0           0          0         0                0  \n",
      "139314        0        0           0          0         0                0  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139315 rows x 72 columns]\n",
      "                    Data_Source    Airline_Name Airline_Type Region_Operation  \\\n",
      "0       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "1       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "2       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "3       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "4       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "...                         ...             ...          ...              ...   \n",
      "139310                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139311                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139312                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139313                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139314                    Kayak  zanair-limited          NaN              NaN   \n",
      "\n",
      "       Aircraft_Type    Cabin_Class Type_Of_Lounge Type_Of_Traveller  \\\n",
      "0               E120  Economy Class            NaN      Solo Leisure   \n",
      "1       Embraer E120  Economy Class            NaN      Solo Leisure   \n",
      "2                NaN  Economy Class            NaN      Solo Leisure   \n",
      "3                NaN  Economy Class            NaN    Couple Leisure   \n",
      "4             CR 900  Economy Class            NaN    Couple Leisure   \n",
      "...              ...            ...            ...               ...   \n",
      "139310           NaN            NaN            NaN               NaN   \n",
      "139311           NaN            NaN            NaN               NaN   \n",
      "139312           NaN            NaN            NaN               NaN   \n",
      "139313           NaN            NaN            NaN               NaN   \n",
      "139314           NaN            NaN            NaN               NaN   \n",
      "\n",
      "       Date_Visit      Date_Flown  ... upsetting unpleasant unhappy  \\\n",
      "0             NaN       June 2019  ...         0          0       0   \n",
      "1             NaN       June 2019  ...         0          0       0   \n",
      "2             NaN  September 2019  ...         0          0       0   \n",
      "3             NaN  September 2019  ...         0          0       0   \n",
      "4             NaN  September 2019  ...         0          0       0   \n",
      "...           ...             ...  ...       ...        ...     ...   \n",
      "139310        NaN             NaN  ...         0          0       0   \n",
      "139311        NaN             NaN  ...         0          0       0   \n",
      "139312        NaN             NaN  ...         0          0       0   \n",
      "139313        NaN             NaN  ...         0          0       0   \n",
      "139314        NaN             NaN  ...         0          0       0   \n",
      "\n",
      "       catastrophic  rude  impolite  shitty  dumb  lame  nb_word_negatif  \n",
      "0                 0     0         0       0     0     0                0  \n",
      "1                 0     0         0       0     0     0                0  \n",
      "2                 0     0         0       0     0     0                0  \n",
      "3                 0     0         0       0     0     0                0  \n",
      "4                 0     0         0       0     0     0                0  \n",
      "...             ...   ...       ...     ...   ...   ...              ...  \n",
      "139310            0     0         0       0     0     0                0  \n",
      "139311            0     0         0       0     0     0                0  \n",
      "139312            0     0         0       0     0     0                0  \n",
      "139313            0     0         0       0     0     0                0  \n",
      "139314            0     0         0       0     0     0                0  \n",
      "\n",
      "[139315 rows x 77 columns]\n",
      "                    Data_Source    Airline_Name Airline_Type Region_Operation  \\\n",
      "0       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "1       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "2       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "3       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "4       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "...                         ...             ...          ...              ...   \n",
      "139310                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139311                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139312                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139313                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139314                    Kayak  zanair-limited          NaN              NaN   \n",
      "\n",
      "       Aircraft_Type    Cabin_Class Type_Of_Lounge Type_Of_Traveller  \\\n",
      "0               E120  Economy Class            NaN      Solo Leisure   \n",
      "1       Embraer E120  Economy Class            NaN      Solo Leisure   \n",
      "2                NaN  Economy Class            NaN      Solo Leisure   \n",
      "3                NaN  Economy Class            NaN    Couple Leisure   \n",
      "4             CR 900  Economy Class            NaN    Couple Leisure   \n",
      "...              ...            ...            ...               ...   \n",
      "139310           NaN            NaN            NaN               NaN   \n",
      "139311           NaN            NaN            NaN               NaN   \n",
      "139312           NaN            NaN            NaN               NaN   \n",
      "139313           NaN            NaN            NaN               NaN   \n",
      "139314           NaN            NaN            NaN               NaN   \n",
      "\n",
      "       Date_Visit      Date_Flown  ... nb_ponct_special ponctspecialonnormal  \\\n",
      "0             NaN       June 2019  ...                0             0.000000   \n",
      "1             NaN       June 2019  ...                1             0.142857   \n",
      "2             NaN  September 2019  ...                1             0.066667   \n",
      "3             NaN  September 2019  ...                1             0.090909   \n",
      "4             NaN  September 2019  ...                0             0.000000   \n",
      "...           ...             ...  ...              ...                  ...   \n",
      "139310        NaN             NaN  ...                0             0.000000   \n",
      "139311        NaN             NaN  ...                0             0.000000   \n",
      "139312        NaN             NaN  ...                0             0.000000   \n",
      "139313        NaN             NaN  ...                0             0.000000   \n",
      "139314        NaN             NaN  ...                0             0.000000   \n",
      "\n",
      "       nb_word_positif nb_word_negatif  crew  staff  steward  security  \\\n",
      "0                    0               0     0      0        0         0   \n",
      "1                    0               0     0      1        0         0   \n",
      "2                    0               0     0      0        0         0   \n",
      "3                    0               0     0      0        0         0   \n",
      "4                    0               2     0      0        0         0   \n",
      "...                ...             ...   ...    ...      ...       ...   \n",
      "139310               0               0     0      0        0         0   \n",
      "139311               1               0     1      0        0         0   \n",
      "139312               0               0     0      0        0         0   \n",
      "139313               0               0     0      0        0         0   \n",
      "139314               0               0     0      0        0         0   \n",
      "\n",
      "        stewardess  nb_word_staff  \n",
      "0                0              0  \n",
      "1                0              0  \n",
      "2                0              0  \n",
      "3                0              0  \n",
      "4                0              0  \n",
      "...            ...            ...  \n",
      "139310           0              0  \n",
      "139311           0              0  \n",
      "139312           0              0  \n",
      "139313           0              0  \n",
      "139314           0              0  \n",
      "\n",
      "[139315 rows x 62 columns]\n",
      "                    Data_Source    Airline_Name Airline_Type Region_Operation  \\\n",
      "0       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "1       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "2       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "3       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "4       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "...                         ...             ...          ...              ...   \n",
      "139310                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139311                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139312                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139313                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139314                    Kayak  zanair-limited          NaN              NaN   \n",
      "\n",
      "       Aircraft_Type    Cabin_Class Type_Of_Lounge Type_Of_Traveller  \\\n",
      "0               E120  Economy Class            NaN      Solo Leisure   \n",
      "1       Embraer E120  Economy Class            NaN      Solo Leisure   \n",
      "2                NaN  Economy Class            NaN      Solo Leisure   \n",
      "3                NaN  Economy Class            NaN    Couple Leisure   \n",
      "4             CR 900  Economy Class            NaN    Couple Leisure   \n",
      "...              ...            ...            ...               ...   \n",
      "139310           NaN            NaN            NaN               NaN   \n",
      "139311           NaN            NaN            NaN               NaN   \n",
      "139312           NaN            NaN            NaN               NaN   \n",
      "139313           NaN            NaN            NaN               NaN   \n",
      "139314           NaN            NaN            NaN               NaN   \n",
      "\n",
      "       Date_Visit      Date_Flown  ... ponctspecialonnormal nb_word_positif  \\\n",
      "0             NaN       June 2019  ...             0.000000               0   \n",
      "1             NaN       June 2019  ...             0.142857               0   \n",
      "2             NaN  September 2019  ...             0.066667               0   \n",
      "3             NaN  September 2019  ...             0.090909               0   \n",
      "4             NaN  September 2019  ...             0.000000               0   \n",
      "...           ...             ...  ...                  ...             ...   \n",
      "139310        NaN             NaN  ...             0.000000               0   \n",
      "139311        NaN             NaN  ...             0.000000               1   \n",
      "139312        NaN             NaN  ...             0.000000               0   \n",
      "139313        NaN             NaN  ...             0.000000               0   \n",
      "139314        NaN             NaN  ...             0.000000               0   \n",
      "\n",
      "       nb_word_negatif nb_word_staff  snack  food  eat  drink  meal  \\\n",
      "0                    0             0      0     0    0      0     0   \n",
      "1                    0             1      0     0    0      0     0   \n",
      "2                    0             0      0     0    0      0     0   \n",
      "3                    0             0      0     0    0      0     0   \n",
      "4                    2             0      0     0    0      0     0   \n",
      "...                ...           ...    ...   ...  ...    ...   ...   \n",
      "139310               0             0      0     0    0      0     0   \n",
      "139311               0             1      0     1    0      0     0   \n",
      "139312               0             0      0     0    0      0     0   \n",
      "139313               0             0      0     0    0      0     0   \n",
      "139314               0             0      0     0    0      0     0   \n",
      "\n",
      "        nb_word_food  \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  0  \n",
      "4                  0  \n",
      "...              ...  \n",
      "139310             0  \n",
      "139311             0  \n",
      "139312             0  \n",
      "139313             0  \n",
      "139314             0  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139315 rows x 63 columns]\n",
      "                    Data_Source    Airline_Name Airline_Type Region_Operation  \\\n",
      "0       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "1       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "2       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "3       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "4       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "...                         ...             ...          ...              ...   \n",
      "139310                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139311                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139312                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139313                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139314                    Kayak  zanair-limited          NaN              NaN   \n",
      "\n",
      "       Aircraft_Type    Cabin_Class Type_Of_Lounge Type_Of_Traveller  \\\n",
      "0               E120  Economy Class            NaN      Solo Leisure   \n",
      "1       Embraer E120  Economy Class            NaN      Solo Leisure   \n",
      "2                NaN  Economy Class            NaN      Solo Leisure   \n",
      "3                NaN  Economy Class            NaN    Couple Leisure   \n",
      "4             CR 900  Economy Class            NaN    Couple Leisure   \n",
      "...              ...            ...            ...               ...   \n",
      "139310           NaN            NaN            NaN               NaN   \n",
      "139311           NaN            NaN            NaN               NaN   \n",
      "139312           NaN            NaN            NaN               NaN   \n",
      "139313           NaN            NaN            NaN               NaN   \n",
      "139314           NaN            NaN            NaN               NaN   \n",
      "\n",
      "       Date_Visit      Date_Flown  ... nb_word_positif nb_word_negatif  \\\n",
      "0             NaN       June 2019  ...               0               0   \n",
      "1             NaN       June 2019  ...               0               0   \n",
      "2             NaN  September 2019  ...               0               0   \n",
      "3             NaN  September 2019  ...               0               0   \n",
      "4             NaN  September 2019  ...               0               2   \n",
      "...           ...             ...  ...             ...             ...   \n",
      "139310        NaN             NaN  ...               0               0   \n",
      "139311        NaN             NaN  ...               1               0   \n",
      "139312        NaN             NaN  ...               0               0   \n",
      "139313        NaN             NaN  ...               0               0   \n",
      "139314        NaN             NaN  ...               0               0   \n",
      "\n",
      "       nb_word_staff nb_word_food  wifi  movie  kid  internet  screen  \\\n",
      "0                  0            0     0      0    0         0       0   \n",
      "1                  1            0     0      0    0         0       0   \n",
      "2                  0            0     0      0    0         0       0   \n",
      "3                  0            0     0      0    0         0       0   \n",
      "4                  0            0     0      0    0         0       0   \n",
      "...              ...          ...   ...    ...  ...       ...     ...   \n",
      "139310             0            0     0      0    0         0       0   \n",
      "139311             1            1     0      0    0         0       0   \n",
      "139312             0            0     0      0    0         0       0   \n",
      "139313             0            0     0      0    0         0       0   \n",
      "139314             0            0     0      0    0         0       0   \n",
      "\n",
      "        nb_word_inflight  \n",
      "0                      0  \n",
      "1                      0  \n",
      "2                      0  \n",
      "3                      0  \n",
      "4                      0  \n",
      "...                  ...  \n",
      "139310                 0  \n",
      "139311                 0  \n",
      "139312                 0  \n",
      "139313                 0  \n",
      "139314                 0  \n",
      "\n",
      "[139315 rows x 64 columns]\n",
      "                    Data_Source    Airline_Name Airline_Type Region_Operation  \\\n",
      "0       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "1       Airline Quality Airline     ab-aviation          NaN              NaN   \n",
      "2       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "3       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "4       Airline Quality Airline   adria-airways          NaN              NaN   \n",
      "...                         ...             ...          ...              ...   \n",
      "139310                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139311                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139312                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139313                    Kayak  yamal-airlines          NaN              NaN   \n",
      "139314                    Kayak  zanair-limited          NaN              NaN   \n",
      "\n",
      "       Aircraft_Type    Cabin_Class Type_Of_Lounge Type_Of_Traveller  \\\n",
      "0               E120  Economy Class            NaN      Solo Leisure   \n",
      "1       Embraer E120  Economy Class            NaN      Solo Leisure   \n",
      "2                NaN  Economy Class            NaN      Solo Leisure   \n",
      "3                NaN  Economy Class            NaN    Couple Leisure   \n",
      "4             CR 900  Economy Class            NaN    Couple Leisure   \n",
      "...              ...            ...            ...               ...   \n",
      "139310           NaN            NaN            NaN               NaN   \n",
      "139311           NaN            NaN            NaN               NaN   \n",
      "139312           NaN            NaN            NaN               NaN   \n",
      "139313           NaN            NaN            NaN               NaN   \n",
      "139314           NaN            NaN            NaN               NaN   \n",
      "\n",
      "       Date_Visit      Date_Flown  ... nb_word_negatif nb_word_staff  \\\n",
      "0             NaN       June 2019  ...               0             0   \n",
      "1             NaN       June 2019  ...               0             1   \n",
      "2             NaN  September 2019  ...               0             0   \n",
      "3             NaN  September 2019  ...               0             0   \n",
      "4             NaN  September 2019  ...               2             0   \n",
      "...           ...             ...  ...             ...           ...   \n",
      "139310        NaN             NaN  ...               0             0   \n",
      "139311        NaN             NaN  ...               0             1   \n",
      "139312        NaN             NaN  ...               0             0   \n",
      "139313        NaN             NaN  ...               0             0   \n",
      "139314        NaN             NaN  ...               0             0   \n",
      "\n",
      "       nb_word_food nb_word_inflight  soft  hard  dirty  comfort  bed  \\\n",
      "0                 0                0     0     0      0        0    0   \n",
      "1                 0                0     0     0      0        0    0   \n",
      "2                 0                0     0     0      0        0    0   \n",
      "3                 0                0     0     0      0        0    0   \n",
      "4                 0                0     0     0      0        0    0   \n",
      "...             ...              ...   ...   ...    ...      ...  ...   \n",
      "139310            0                0     0     0      0        0    0   \n",
      "139311            1                0     0     0      0        0    0   \n",
      "139312            0                0     0     0      0        0    0   \n",
      "139313            0                0     0     0      0        0    0   \n",
      "139314            0                0     0     0      0        0    0   \n",
      "\n",
      "        nb_word_seat  \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  0  \n",
      "4                  0  \n",
      "...              ...  \n",
      "139310             0  \n",
      "139311             0  \n",
      "139312             0  \n",
      "139313             0  \n",
      "139314             0  \n",
      "\n",
      "[139315 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "# Feature creation\n",
    "\n",
    "all_data_txt_V1 = nb_word_comment(all_data_txt_V1, 'Review')\n",
    "all_data_txt_V1 = sentiment_analyse(all_data_txt_V1, 'Review')\n",
    "all_data_txt_V1 = nb_sentence(all_data_txt_V1, 'Review')\n",
    "all_data_txt_V1 = nb_word_comment(all_data_txt_V1, 'Review')\n",
    "all_data_txt_V1[\"meanword\"] = all_data_txt_V1[\"nb_word_comment\"] / \\\n",
    "    all_data_txt_V1[\"nbsentence\"]\n",
    "all_data_txt_V1 = nblisteponctuation(\n",
    "    all_data_txt_V1, punctuation_list, \"Review\", \"normal\")\n",
    "all_data_txt_V1 = nblisteponctuation(\n",
    "    all_data_txt_V1, special_list, \"Review\", \"special\")\n",
    "all_data_txt_V1[\"ponctspecialonnormal\"] = all_data_txt_V1[\"nb_ponct_special\"] / \\\n",
    "    all_data_txt_V1[\"nb_ponct_normal\"]\n",
    "\n",
    "all_data_txt_V1 = nblistword(all_data_txt_V1, list_positive, 'Review', 'positif')\n",
    "all_data_txt_V1 = nblistword(all_data_txt_V1, list_negative, 'Review', 'negatif')\n",
    "all_data_txt_V1 = nblistword(all_data_txt_V1, list_Staff, 'Review', 'staff')\n",
    "all_data_txt_V1 = nblistword(all_data_txt_V1, list_Food, 'Review', 'food')\n",
    "all_data_txt_V1 = nblistword(all_data_txt_V1, list_Inflight, 'Review', 'inflight')\n",
    "all_data_txt_V1 = nblistword(all_data_txt_V1, list_Seat, 'Review', 'seat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:07:34.862523Z",
     "start_time": "2020-01-17T13:07:34.756854Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing data for model execution\n",
    "\n",
    "# Deleting the empty rows\n",
    "\n",
    "preprocessed_txt_V1 = preprocessed_txt_V1[preprocessed_txt_V1[\"commentaire\"].notnull()]\n",
    "preprocessed_txt_V1.drop([\"commentaire\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:07:35.521132Z",
     "start_time": "2020-01-17T13:07:35.468016Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the dataframe with all labels and the features but the tf-idf\n",
    "\n",
    "all_data_txt_V1 = pd.DataFrame(all_data_txt_V1[[\"Seat_Comfort\", \"Food_And_Beverages\", \"Cabin_Staff_Service\", \"Inflight_Entertainment\", 'nb_word_comment', 'polarity', 'subjectivity', 'neg', 'neu', 'pos', 'compound', 'nbsentence', 'meanword', 'nb_ponct_normal', 'nb_ponct_special', 'ponctspecialonnormal', 'nb_word_positif', 'nb_word_negatif', 'nb_word_staff', 'nb_word_food', 'nb_word_inflight', 'nb_word_seat']], columns=[\n",
    "                           \"Seat_Comfort\", \"Food_And_Beverages\", \"Cabin_Staff_Service\", \"Inflight_Entertainment\", 'nb_word_comment', 'polarity', 'subjectivity', 'neg', 'neu', 'pos', 'compound', 'nbsentence', 'meanword', 'nb_ponct_normal', 'nb_ponct_special', 'ponctspecialonnormal', 'nb_word_positif', 'nb_word_negatif', 'nb_word_staff', 'nb_word_food', 'nb_word_inflight', 'nb_word_seat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:07:41.141679Z",
     "start_time": "2020-01-17T13:07:40.812539Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merging of the dataframes and converting the rating to a scale from 1 to 3\n",
    "\n",
    "preprocessed_txt_V1 = pd.concat([preprocessed_txt_V1, all_data_txt_V1], axis=1)\n",
    "preprocessed_txt_V1.replace({2: 1, 3: 2, 4: 3, 5: 3}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:07:42.536736Z",
     "start_time": "2020-01-17T13:07:42.207807Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selecting the different labels in order to process them with the different models\n",
    "\n",
    "data_seat = preprocessed_txt_V1.drop(\n",
    "    [\"Food_And_Beverages\", \"Cabin_Staff_Service\", \"Inflight_Entertainment\"], axis=1)\n",
    "\n",
    "data_food = preprocessed_txt_V1.drop(\n",
    "    [\"Seat_Comfort\", \"Cabin_Staff_Service\", \"Inflight_Entertainment\"], axis=1)\n",
    "\n",
    "data_staff = preprocessed_txt_V1.drop(\n",
    "    [\"Food_And_Beverages\", \"Seat_Comfort\", \"Inflight_Entertainment\"], axis=1)\n",
    "\n",
    "data_inflight = preprocessed_txt_V1.drop(\n",
    "    [\"Food_And_Beverages\", \"Seat_Comfort\", \"Cabin_Staff_Service\"], axis=1)\n",
    "\n",
    "\n",
    "# Deleting the missing values from the labels and filling in the Non available with 0\n",
    "\n",
    "data_seat = data_seat[data_seat[\"Seat_Comfort\"].notnull()]\n",
    "data_seat.fillna(0, inplace=True)\n",
    "data_food = data_food[data_food[\"Food_And_Beverages\"].notnull()]\n",
    "data_food.fillna(0, inplace=True)\n",
    "data_staff = data_staff[data_staff[\"Cabin_Staff_Service\"].notnull()]\n",
    "data_staff.fillna(0, inplace=True)\n",
    "data_inflight = data_inflight[data_inflight[\"Inflight_Entertainment\"].notnull(\n",
    ")]\n",
    "data_inflight.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:08:04.478536Z",
     "start_time": "2020-01-17T13:07:44.409248Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_seat.to_csv(path + \"g5_seat_V1.csv\", index = False)\n",
    "data_food.to_csv(path + \"g5_food_V1.csv\", index = False)\n",
    "data_staff.to_csv(path + \"g5_staff_V1.csv\", index = False)\n",
    "data_inflight.to_csv(path + \"g5_inflight_V1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Metadata analysis approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first version is a test of what could be done as a pre-processing. It contains simple treatments like dropping columns and filling with median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T15:50:44.132645Z",
     "start_time": "2020-01-16T15:50:44.076498Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_MD_V0 = all_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T15:50:44.617215Z",
     "start_time": "2020-01-16T15:50:44.557135Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping some columns\n",
    "all_data_MD_V0.drop([\"Type_Of_Traveller\", \"Date_Visit\", \"Date_Flown\", \"Airport\",\n",
    "                                \"Route\", \"Category\", \"Category_Detail\", \"Date_Review\",\n",
    "                                \"Review\", \"Type_Of_Lounge\", \"Airline_Name\", \"Aircraft_Type\",\n",
    "                                \"Seat\", \"Overall_Customer_Rating\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T15:50:45.218167Z",
     "start_time": "2020-01-16T15:50:45.001726Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replacing missing values\n",
    "for i in all_data_MD_V0.columns:\n",
    "    try : \n",
    "        all_data_MD_V0[i].fillna(all_data_MD_V0[i].median(), inplace=True)\n",
    "    except : \n",
    "        all_data_MD_V0[i].fillna('Empty', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T15:50:45.774756Z",
     "start_time": "2020-01-16T15:50:45.602527Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforming qualitative data into quantitative ones\n",
    "all_data_MD_V0 = pd.get_dummies(all_data_MD_V0, columns=[\"Data_Source\", \"Airline_Type\",\n",
    "                                       \"Region_Operation\",\n",
    "                                       \"Cabin_Class\",\n",
    "                                       \"Seat_Type\", \"Recommended\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T15:50:55.147231Z",
     "start_time": "2020-01-16T15:50:46.123221Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_MD_V0.to_csv(path + 'all_data_MD_V0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:02:36.096683Z",
     "start_time": "2020-01-17T09:02:36.048554Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_V1_MD = all_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before completing columns, we analyze them. We drop columns if :\n",
    "- they're empty.\n",
    "- they've too many different values. It would be hard to complete them and predict with them.\n",
    "\n",
    "We chose the DataSource 'Airline Quality Airline' because it was the most complete data source. <br/> Moreover, we wanted to predict some label that weren't filled in other data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:02:36.959978Z",
     "start_time": "2020-01-17T09:02:36.510785Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_V1_MD = all_data_V1_MD[all_data_V1_MD['Data_Source']\n",
    "                                == 'Airline Quality Airline']\n",
    "list_to_drop = ['Airline_Type', 'Region_Operation', 'Review',\n",
    "                'Date_Review', 'Catering', 'Cleanliness',\n",
    "                'Wifi_And_Connectivity', 'Type_Of_Lounge',\n",
    "                'Lounge_Staff_Service', 'Category', 'Category_Detail',\n",
    "                'Ground_Service', 'Lounge_Comfort', 'Seat_Legroom',\n",
    "                'Seat_Storage', 'Seat_Recline', 'Viewing_Tv_Screen',\n",
    "                'Power_Supply', 'Seat', 'Seat_Width', 'Washrooms',\n",
    "                'Aisle_Space', 'Overall_Service_Rating', 'Overall_Airline_Rating',\n",
    "                'Bar_And_Beverages', 'Route', 'Airport']\n",
    "\n",
    "for column in list_to_drop:\n",
    "    del all_data_V1_MD[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fill empty columns, we used different technics : \n",
    "- khi² independance between every columns. If one column matchs, we complete the original column with an adapted rules.\n",
    "- Filling with the mode (if that makes sense on the variable)\n",
    "- Using KNN to predict the Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:05:29.988554Z",
     "start_time": "2020-01-17T09:02:37.162641Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete \"Cabin_Class\" column\n",
    "all_data_V1_MD = clean_Cabin_Class(all_data_V1_MD)\n",
    "\n",
    "# Useless column\n",
    "del all_data_V1_MD['Seat_Type']\n",
    "\n",
    "# Column Recommended\n",
    "all_data_V1_MD['Recommended'] = all_data_V1_MD['Recommended'].fillna('no')\n",
    "\n",
    "#Complete \"Value_For_Money\" column\n",
    "all_data_V1_MD = clean_Value_For_Money(all_data_V1_MD)\n",
    "\n",
    "# Column Type_Of_Traveller\n",
    "all_data_V1_MD = clean_Type_Of_Traveller(all_data_V1_MD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted more caracteristics to predict labels. \n",
    "<br/> We had access to a file that was coming from Seat Guru website. This fils contains many data about seats and cabin in general. \n",
    "<br/> This data will then be added on the DataFrame we were working on (by homogenizing key columns and then merging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:05:31.897320Z",
     "start_time": "2020-01-17T09:05:30.164940Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_V1_MD = homogenize_aircraft(all_data_V1_MD)\n",
    "aircraft = homogenize_aircraft(aircraft)\n",
    "all_data_V1_MD = homogenize_cabin_class(all_data_V1_MD, 'Cabin_Class')\n",
    "aircraft = homogenize_cabin_class(aircraft, 'Category')\n",
    "all_data_V1_MD = homogenize_airline(all_data_V1_MD, 'Airline_Name')\n",
    "aircraft = homogenize_airline(aircraft, 'Airline_name')\n",
    "all_data_V1_MD = merge_all_data_seatguru(all_data_V1_MD, aircraft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T16:50:35.887782Z",
     "start_time": "2020-01-16T16:49:36.417Z"
    },
    "collapsed": true
   },
   "source": [
    "For this version we will wait untill further pre processing to build models,in order to get best results and improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:05:32.106561Z",
     "start_time": "2020-01-17T09:05:32.080503Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_V1_MD['Overall_Customer_Rating'] = all_data_V1_MD['Overall_Customer_Rating'].replace(\n",
    "    'na', -1).astype(int).replace(-1, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:05:32.347198Z",
     "start_time": "2020-01-17T09:05:32.330154Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_temporaire = all_data_V1_MD[['Food_And_Beverages', 'Inflight_Entertainment',\n",
    "                                'Overall_Customer_Rating', 'Cabin_Staff_Service', 'Seat_Comfort']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:05:32.597866Z",
     "start_time": "2020-01-17T09:05:32.577814Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_temporaire_original = all_data_V1_MD.drop(\n",
    "    ['Food_And_Beverages', 'Inflight_Entertainment', 'Overall_Customer_Rating', 'Cabin_Staff_Service', 'Seat_Comfort'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:05:32.888670Z",
     "start_time": "2020-01-17T09:05:32.860569Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del all_data_V1_MD['Food_And_Beverages']\n",
    "del all_data_V1_MD['Inflight_Entertainment']\n",
    "del all_data_V1_MD['Overall_Customer_Rating']\n",
    "del all_data_V1_MD['Cabin_Staff_Service']\n",
    "del all_data_V1_MD['Seat_Comfort']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns contain Nan Values. \n",
    "<br/> So, we complete them with a logistic regression with those steps :  \n",
    "- Testing a logistic regression on all columns\n",
    "- Getting the best score of prediction\n",
    "- Filling the concerned column by logistic regression\n",
    "- Repeat. \n",
    "\n",
    "<br/> If all columns are complete, the linear regression can be usefull if we merge with new DataSource or new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:05:33.430145Z",
     "start_time": "2020-01-17T09:05:33.149332Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_V1_MD['Recline'] = all_data_V1_MD['Recline'].fillna('nc')\n",
    "all_data_V1_MD['Recline'] = all_data_V1_MD['Recline'].apply(\n",
    "    get_recline_percent)\n",
    "all_data_V1_MD['Recline'] = all_data_V1_MD['Recline'].replace('nc', np.NaN)\n",
    "all_data_V1_MD['Recline'] = all_data_V1_MD['Recline'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:05:33.848261Z",
     "start_time": "2020-01-17T09:05:33.656757Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear regression for complete last Nan values\n",
    "all_data_V1_MD = select_col_nal(all_data_V1_MD, 100)\n",
    "all_data_V1_MD = select_lign_nal(all_data_V1_MD, 100)\n",
    "all_data_V1_MD = select_moda_nal(all_data_V1_MD,15)\n",
    "\n",
    "try:\n",
    "    all_data_V1_MD, tab_score = fill_dataset(all_data_V1_MD)\n",
    "except ValueError:\n",
    "    print(\"Columns already completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:05:34.085892Z",
     "start_time": "2020-01-17T09:05:34.053810Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_V1_MD = pd.merge(all_data_V1_MD, all_data_temporaire_original,\n",
    "                    left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:05:34.393712Z",
     "start_time": "2020-01-17T09:05:34.352602Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the original data with regression linear\n",
    "\n",
    "columns = all_data_V1_MD.columns\n",
    "\n",
    "all_data_V1_MD = pd.merge(all_data_V1_MD, all_data_temporaire,\n",
    "                    left_index=True, right_index=True)\n",
    "all_data_temporaire_original = all_data_temporaire_original.drop(\n",
    "    columns, axis=1)\n",
    "all_data_V1_MD = pd.merge(all_data_V1_MD, all_data_temporaire_original,\n",
    "                    left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No new features in this version (only merging that occured in part V1.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:12:30.952879Z",
     "start_time": "2020-01-17T09:12:30.250954Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_V1_MD.to_csv(path + 'all_data_MD_V1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, the only difference is feature engineering, because we considered that our pre process was optimal according to our results on models. Then we will use the DataFrame of previous version to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:12:32.638297Z",
     "start_time": "2020-01-17T09:12:32.408687Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_V2_MD = pd.read_csv(path + 'all_data_MD_V1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not present in this version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not present in this version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create more features in order to have better performances on our models. Those features are : \n",
    "- Decomposition of a flying date in month and year\n",
    "- Creating 2 new columns width min and max in order to solve the issue of interval width\n",
    "- Filling Percentage of considered flight\n",
    "- Difference between average overall rating and customer overall rating (is the customer close to others ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:12:53.355968Z",
     "start_time": "2020-01-17T09:12:51.324559Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_V2_MD[['Month_Flight', 'Year_Flight']\n",
    "               ] = all_data_V2_MD['Date_Flown'].str.split(\" \", expand=True,)\n",
    "all_data_V2_MD = all_data_V2_MD.drop(['Date_Flown'], axis=1)\n",
    "all_data_V2_MD[['Width_Min', 'Width_Max']\n",
    "               ] = all_data_V2_MD['Width'].str.split(\"-\", expand=True,)\n",
    "all_data_V2_MD = all_data_V2_MD.drop(['Width'], axis=1)\n",
    "all_data_V2_MD['Width_Max'] = all_data_V2_MD.apply(lambda x: fill_width_max(\n",
    "    x['Width_Min'], x['Width_Max']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:12:54.652502Z",
     "start_time": "2020-01-17T09:12:53.658776Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_V2_MD[\"Filling_Percent\"] = all_data_V2_MD.apply(lambda x: filling_percent(\n",
    "    x['Total_seat'], x['Count']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:12:56.018750Z",
     "start_time": "2020-01-17T09:12:54.915116Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_V2_MD['Rating_Overall_Rating_Gap'] = all_data_V2_MD.apply(\n",
    "    lambda x: rating_overall_rating_gap(x['Rating'], x['Overall_Customer_Rating']), axis=1)\n",
    "all_data_V2_MD = all_data_V2_MD.drop(['Date_Visit'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:14:57.260326Z",
     "start_time": "2020-01-17T09:14:56.441219Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_V2_MD.to_csv(path + 'all_data_MD_V2.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "270px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
